\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Parallel Compilation in the Cloud}

\author{\IEEEauthorblockN{Jamie Willis}
\IEEEauthorblockA{\textit{School of Computer Science} \\
\textit{University of Bristol}\\
jw14896@bristol.ac.uk}
\and
\IEEEauthorblockN{Samuel Russell}
\IEEEauthorblockA{\textit{School of Computer Science} \\
\textit{University of Bristol}\\
sr14978@bristol.ac.uk}
}

\maketitle

\begin{abstract}
We present an application based in the cloud allowing users to compile large C
or C++ projects in parallel, utilising Google App Engine. The application can
compile projects by allowing a user to upload a flattened zip with the source
code and headers. It is available online at \emph{LINK HERE}.
\end{abstract}
\section{Introduction}
Large C or C++ projects often take a very long time to compile, and this is a
large waste of time for developers. Many compilers are not multi-threaded,
since the process of compilation and optimisation must be done in an ordering with
many dependencies. Despite this, however, each file in itself is completely
orthogonal to every other file (until link-time). This results in an ability to
compile multiple files at once by using scripts that launch many compiler
instances in separate threads. This is a good improvement on the compilation
process, but it is limited by the number of processor cores you have.
Additionally, if your compiler \emph{is} multi-threaded, this will place a large
amount of strain on the system.

Our solution is to leverage multiple processors, distributed across the cloud,
each of which can handle the compilation of one or many files depending on how
big the files are. This can prove to be a large advantage for much larger
projects, that might otherwise take over half an hour to compile! This technique
also allows us to leverage multi-threaded compilers whilst also compiling files
in parallel.

% potentially add in more information?

\section{Implementation}
\subsection{Overall Procedure}
When a user uploads a zip file and submits the corresponding compiler and linker
flags to the app, the following process must be undertaken; firstly, the zip
file is unpacked server-side and the GCC macro pre-processor is ran over each of
the source files. This allows us to discard the header files, since they have
already been inserted, at the cost of file size. 

Secondly, the files are uploaded onto the ``blob'' storage Google App Engine
provides. All of the files are then placed into the work queue requesting
compilation, and, additionally, a request for linking is placed onto the queue.

Thirdly, various micro-service processors consume work from the queue; they
download the relevant source file, compile it and upload the resulting file or
error messages to the blob. At the same time, the linker process is waiting for
all files to be finished and, if they were all successful, links the
application and uploads the executable or coagulates the error messages and
submits them back to the end user.
\subsection{Underlying Architecture}

The solution consists of a collection of independent micro services, each managed separately by Google App engine. We used this platform-as-a-service (PaaS) offering to take advantage of Google Flex environment which automatically scales and load balances. Each micro-service is written in python using the Flask web framework. 

- There is a front end service which provides access to users though web pages including static and dynamic content.

- The site web-app makes Ajax requests to the API service, which provides access to all the functions through a rest-full interface.

- The third micro-service conducts a map-then-reduce process to compile the given source code and store it to object storage. We could have created 3 sub services to operate the three actions needed but we opted to use one micro-service since this means the workers are more flexible to complete any of the actions and so fewer instances will be needed for light loads.
\\\\
To store information about users we have used Google's cloud Datastore offering. All micro-services communicate with the Datastore through the Google's client API.
\\\\
All files are stored in Google's Cloud Object storage offering that provides a unified place for data across the micro-services.
\\\\
The API service is decoupled from the worker service using a Google's PubSub queue. We use a push queue that instigates the jobs by submitting POST requests to the worker service and handles re-scheduling if the  job is not completed.

\section{Scalability}

Vertical scaling is increasing the size of you compute devices to get better performance. This normally means the software architecture does not need to be changed, however, there are limits to the size of servers and also, using many smaller commodity servers is cheaper due to efficiencies of scale. Therefore, the current trend is to use the horizontal scaling of using more machines to satisfy high demand.
\\\\
There are 3 axis of the horizontal scaling cube, and Google App Engine implements them all.

The first way of scaling out is functional decomposition, which we have done by splitting our system in to three micro-services. This creates good software maintainability since each part of the process is decoupled from the next and so has clear, well-defined interaction points.

The second is data-partitioning, which can be geographical or purely over separate user demographics. Since, within our application users do not share data and usually with remain in one location for sustained periods assigning resources in close proximity to them will increase performance because for instance caches become more efficient. GAE, will create multiple server instances from our template in different data centres and can route traffic to the most applicable one. 

Finally, within a specific feature and geographic server group there can be instance replication to serve the load. This replicated instance group can also be scaled up and down to match demand as close as possible. GAE support three types of scaling: Manual, Basic and Automatic. In manual scaling mode, the service administrator or management software decides when to increase or decrease the number of instances. The instances have a persisting state and are good for long running jobs but aren't as reactive to change in load. Basic scaling, only creates a new instance when the application receives a request and turns it down again after it has finished. This is targeted towards very intermittent, small-scale use because it uses minimal resources but has the overhead of starting up new instances for new requests. Automatic scaling uses application metrics such as request latencies to gauge scaling demands. We use GAE automatic scaling between 1 and 16 instances based on the CPU utilisation to provide a capable but responsive system. 

You can also host multiple versions of software and do incremental feature rollouts by using traffic splitting to gradually upgrade the service with minimal disruption.


\section{Future Work and Limitations}
\subsection{Linux Only Compilation}
A severe current limitation of our system is that it only supported Linux
compilation, since the server we are provided with is a Linux server. In order
to compile for Windows and other platforms, we would need investigate the use of
MinGW on Linux in order to produce Windows binaries. For this proof of concept,
however, this is not deemed to be a requirement.
\subsection{More Languages}
Currently, only C and C++ are supported languages. However, it would be possible
in future work to include more languages that suffer from the same long
compilation times. The limitation would be on what compilers are installed on
the server, and whether or not we could install new ones.
\subsection{Non-nested Zip Structure}
A current limitation with the design is the requirement of a flattened zip
structure for the uploaded source code. Ideally in future the app would be able
to support any folder structure inside the zip and create the appropriate
structure in the blob storage. This should not be too difficult to achieve, but
we decided it does not add much to the application given that the main focus is
on the parallel aspect.
\section{Conclusion}
%\begin{thebibliography}{00}

%\end{thebibliography}
\end{document}