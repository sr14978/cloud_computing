\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{url}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Parallel Compilation in the Cloud}

\author{\IEEEauthorblockN{Jamie Willis}
\IEEEauthorblockA{\textit{School of Computer Science} \\
\textit{University of Bristol}\\
jw14896@bristol.ac.uk}
\and
\IEEEauthorblockN{Samuel Russell}
\IEEEauthorblockA{\textit{School of Computer Science} \\
\textit{University of Bristol}\\
sr14978@bristol.ac.uk}
}

\maketitle

\begin{abstract}
We present an application based in the cloud allowing users to compile large C
or C++ projects in parallel, utilising Google App Engine. The application can
compile projects by allowing a user to upload a flattened zip with the source
code and headers. It is available online at
\url{https://cloudcomputingcompliler.appspot.com}.
\end{abstract}
\section{Introduction}
In industry, very large C or C++ projects are common. These can take a very long time to compile, and this is a
large waste of time for developers. Many compilers are not multi-threaded,
since the process of compilation and optimisation must be done in an ordering with
many dependencies. Despite this, however, each file in itself is completely
orthogonal to every other file (until link-time). This results in an ability to
compile multiple files at once by using scripts that launch many compiler
instances in separate threads. This is a good improvement on the compilation
process, but it is limited by the number of processor cores you have.
Additionally, if your compiler \emph{is} multi-threaded, this will place a large
amount of strain on the system.

Our solution is to leverage multiple processors, distributed across the cloud,
each of which can handle the compilation of one or many files depending on how
big the files are. This can prove to be a large advantage for much larger
projects, that might otherwise take over half an hour to compile! This technique
also allows us to leverage multi-threaded compilers whilst also compiling files
in parallel.

% potentially add in more information?

\section{Implementation}
\subsection{Overall Procedure}
A user visits the website provided by the front end service. They send a zip file along with the corresponding compiler and linker flags to the API using the webapp.

Then, in the cloud, the zip file is tagged and uploaded to object storage, a task is submitted to the queue to initiate the compilation and the tag is returned to the user app for retrieval of the results.

Upon completion of the compilation, the results file and executable is uploaded to blob storage and becomes available for the app to download.

The compilation process consists of three steps:
Firstly, the zip file is unpacked and the GCC macro pre-processor is ran over each of
the source files to include all necessary definitions between them. This allows us to discard the header files, since they have already been inserted, at the cost of source file size. These source files are then uploaded to object storage individually and the zip file is deleted as it is no longer needed. A task for compilation of each file is then submitted to the queue along with a task to link the resulting object files together. 

Secondly, for each compilation task of a source file, the file is downloaded from object storage and compiled locally. The object file, along with any warnings and errors is then collected and stored back to object storage.

Thirdly, the linker checks that all files have been compiled then, if they were
all successful, downloads them in to a local folder before linking them together
in to an executable. If the objects file are not all ready it reschedules
itself and returns. Finally, it uploads the executable or coagulated error messages to be available to the user.

\subsection{Underlying Architecture}
\begin{figure}[ht] % ht
    \centering
    \includegraphics[width=3.2in]{OverallArchitecture.png}
    \caption{Overall architecture}
    \label{fig:overview}
\end{figure}

Our solution consists of a collection of independent micro services, each
managed separately by Google App engine. We used this platform-as-a-service (PaaS) offering to take advantage of Google Flex environment which automatically scales and load balances \cite{GAE}. Each micro-service is written in Python using the Flask web framework \cite{Flask}:

\begin{itemize}
\item A Front end service which provides access to users though web
pages including static and dynamic content.

\item An API service, which provides
access to all the functions through a RESTful interface.

\item The third micro-service conducts a \emph{map-then-reduce} process to
compile the given source code and store it to object storage. 
\end{itemize}

We could have created three
sub services to operate the three actions needed but we opted to use one micro-service since this means the workers are more flexible to complete any of the actions and so fewer instances will be needed for light loads.

The static content to be served to the user is cached in a geographically distributed fashion on Google's Content Management System. This did cause some issues in testing since when uploading a new version, the old code was still present in the cache so the content did not update. However, in production this improves the latency of resource retrieval greatly.

To store information about users we have used Google's cloud Datastore offering. All micro-services communicate with the Datastore through the Google's client API. Datastore is a non-relational database so structured queries such as table joins are not available. However, it is great for storing simple variable types as this means is able to be massively scalable and deliver high performance since it uses a distributed architecture, and scales automatically. Distributed database's have need to consider consistency and Datastore uses strong consistency when querying keys but eventual consistency for other content based queries. This is fine for our application's needs since it is not critical data. It also guarantees atomic transactions encrypts are data written to disk.

All files are stored in Google's Cloud Object storage offering that provides a unified place for data across the micro-services. Object storage differs from other storage architectures like hierarchical file-systems in that it consists of an unstructured collection of variable sized object blobs each retrievable by a globally unique identifier (in Google Cloud storage you can create folders, but they are just abstractions on top of globally unique names). We created our own object storage bucket to store our zip files, source code, object code and compiler messages. 

The API service is decoupled from the worker service using a Google's PubSub queue. We use a push queue that instigates the jobs by submitting POST requests to the worker service. It can protect the workers from large surges in demand using flow control dynamic rate limiting. Also, it improves robustness since it guarantees at-least-once messaging by re-scheduling failed jobs.

\section{Scalability}

Vertical scaling is increasing the size of you compute devices to get better performance. This normally means the software architecture does not need to be changed, however, there are limits to the size of servers and also, using many smaller commodity servers is cheaper due to efficiencies of scale. Therefore, the current trend is to use the horizontal scaling of using more machines to satisfy high demand.

There are three axis of the horizontal scaling cube, and Google App Engine
implements them all.

The first way of scaling out is functional decomposition, which we have done by splitting our system in to three micro-services. This creates good software maintainability since each part of the process is decoupled from the next and so has clear, well-defined interaction points.

The second is data-partitioning, which can be geographical or purely over separate user demographics. Since, within our application users do not share data and usually with remain in one location for sustained periods assigning resources in close proximity to them will increase performance because for instance caches become more efficient. GAE, will create multiple server instances from our template in different data centres and can route traffic to the most applicable one. 

Finally, within a specific feature and geographic server group there can be
instance replication to serve the load. This replicated instance group can also
be scaled up and down to match demand as close as possible. GAE support three
types of scaling: Manual, Basic and Automatic. In manual scaling mode, the
service administrator or management software decides when to increase or
decrease the number of instances. The instances have a persisting state and are
good for long running jobs but aren't as reactive to change in load. Basic
scaling, only creates a new instance when the application receives a request and
turns it down again after it has finished. This is targeted towards very
intermittent, small-scale use because it uses minimal resources but has the
overhead of starting up new instances for new requests. Automatic scaling uses application metrics such as request latencies to gauge scaling demands. We use GAE automatic scaling between one and sixteen instances based on the CPU utilisation to provide a capable but responsive system.

You can also host multiple versions of software and do incremental feature rollouts by using traffic splitting to gradually upgrade the service with minimal disruption.


\section{Future Work and Limitations}
\subsection{Linux Only Compilation}
A severe current limitation of our system is that it only supported Linux
compilation, since the server we are provided with is a Linux server. In order
to compile for Windows and other platforms, we would need investigate the use of
MinGW on Linux in order to produce Windows binaries. For this proof of concept,
however, this is not deemed to be a requirement.
\subsection{More Languages}
Currently, only C and C++ are supported languages. However, it would be possible
in future work to include more languages that suffer from the same long
compilation times. The limitation would be on what compilers are installed on
the server, and whether or not we could install new ones.
\subsection{Non-nested Zip Structure}
A current limitation with the design is the requirement of a flattened zip
structure for the uploaded source code. Ideally in future the app would be able
to support any folder structure inside the zip and create the appropriate
structure in the blob storage. This should not be too difficult to achieve, but
we decided it does not add much to the application given that the main focus is
on the parallel aspect.
\section{Conclusion}

\begin{thebibliography}{00}
\bibitem{GAE}
    Google App Engine: A PaaS offering auto scaling instances
    \url{https://cloud.google.com/appengine/docs/flexible/}

\bibitem{Flask}
    Flask: a web microframework for Python
    \url{http://flask.pocoo.org/}
\end{thebibliography}

\end{document}